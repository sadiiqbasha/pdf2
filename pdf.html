<html>
<head>
    <title>Pdf.converted</title>
    <link rel="stylesheet" href="style.css">
</head>
    <body>
        <div>
            <h2>Conclusions</h2>
            <p>
            In this section, we propose a new two-level ensemble construction algorithm, MetaEvolutionary Ensembles (MEE), that uses feature selection as the diversity mechanism.At the first level, individual classifiers compete against each other to correctly predictheld-out examples. Classifiers are rewarded for predicting difficult points, relative to theother members of their respective ensembles. At the top level, the ensembles competedirectly based on classification accuracy.<br>
                
            </p>
        </div>  
        <div class="1">
        <p>
            Our model shows consistently improved classification performance compared to a
single classifier at the cost of computational complexity. Compared to the traditional
ensembles (Bagging and Boosting) and GEFS, our resulting ensemble shows comparable
performance while maintaining a smaller ensemble. Further, our two-level evolutionary
framework confirms that more diversity among classifiers can improve predictive accuracy. Up to a certain level, the ensemble size also has a positive effect on the ensemble
performance.
            </p>
        </div>
        <div>
        <p>
           The next step is to compare this algorithm more rigorously to others on a larger
collection of data sets, and perform any necessary performance tweaks on the EA energy
allocation scheme. This new experiment is to test the claim that there is relatively little
room for other ensembles algorithm to obtain further improvement over decision forest
method (Breiman, 1999). Along the way, we will examine the role of various characteristics
of ensembles (size, diversity, etc.) and classifiers (type, number of dimensions/data
points, etc.). By giving the system as many degrees of freedom as possible and observing
the characteristics that lead to successful ensembles, we can directly optimize these
characteristics and translate the results to a more scalable architecture (Street & Kim,
2001) for large-scale predictive tasks. 
            </p>
        </div>
<div class="h2">
        <h2>CONCLUSIONS</h2>
        </div>
        <div>
        In this chapter, we proposed a new framework for feature selection in supervised
and unsupervised learning. In particular, we note that each feature subset should be
evaluated in terms of multiple objectives. In supervised learning, ELSA with neural
networks model (ELSA/ANN) was used to search for possible combinations of features
and to score customers based on the probability of buying new insurance product
respectively. The ELSA/ANN model showed promising results in two different experiments, when market managers have clear decision scenario or when they don’t. ELSA
was also used for unsupervised feature selection. Our algorithm, ELSA/EM, outperforms
a greedy algorithm in terms of classification accuracy. Most importantly, in the proposed
framework we can reliably select an appropriate clustering model, including significant
features and the number of clusters.
        </div><br>
        <div>
        We also proposed a new ensemble construction algorithm, Meta-Evolutionary
Ensembles (MEE), where feature selection is used as the diversity mechanism among
classifiers in the ensemble. In MEE, classifiers are rewarded for predicting difficult
points, relative to the other members of their respective ensembles. Our experimental
results indicate that this method shows consistently improved performance compared
to a single classifier and the traditional ensembles.
        </div>
        <div><br>
        One major direction of future research on the feature selection with ELSA is to find
a way to boost the weak selection pressure of ELSA while keeping its local selection
mechanism. For problems requiring effective selection pressure, local selection may be
too weak because the only selection pressure that ELSA can apply comes from the
sharing of resources. Dynamically adjusting the local environmental structure based on
the certain ranges of the observed fitness values over a fixed number of generations could
be a promising solution. In this way, we could avoid the case in which the solution with
the worst performance can survive into the next generation because there are no other
solutions in its local environment.
        </div><br>
        <div>
        Another major direction of future research is related with the scalability issue. By
minimizing the communication among agents, our local selection mechanism makes ELSA
efficient and scalable. However, our models suffer the inherent weakness of the wrapper
model, the computational complexity. Further by combining EAs with ANN to take the
advantages of both algorithms, it is possible that the combined model can be so slow that
it cannot provide solutions in a timely manner. With the rapid growth of records and
variables in database, this failure can be critical. Combining ELSA with faster learning
algorithms such as decision tree algorithms and Support Vector Machine (SVM) will be
worth pursuing.
        </div>
        <div class="hi3">
        <h2>ENDNOTES</h2>
         <p>
            1 Continuous objective functions are discretized.<br>
            2 This is one of main tasks in the 2000 CoIL challenge (Kim & Street, 2000). For   more information about CoIL challenges and the data sets, please refer to         http://www.dcs.napier.ac.uk/coil/challenge/.<br>
            3 If other objective values are equal, we prefer to choose a solution with         smallvariance. <br>  
            4 This is reasonable because as we select more prospects, the expected accuracy     gain will go down. If the marginal revenue from an additional prospect is much   greater than the marginal cost, however, we could sacrifice the expected         accuracy gain.Information on mailing cost and customer value was not available   in this study. <br>
           5 The other four features selected by the ELSA/logit model are: contribution to
               bicycle policy, fire policy, number of trailer, and lorry policies.<br> 
           6 The cases of zero or one cluster are meaningless, therefore we count the           number of clusters as K = κ + 2 where κ is the number of ones and 
             Kmin = 2 ≤ K ≤ Kmax.<br> 
           7 For K = 2, we use Fcomplexity = 0.76, which is the closest value to 0.69           represented in the front. <br> 
           8 In our experiments, standard error is computed as standard deviation /             iter0.5where iter = 5.<br> 
            </p>   
        </div>
        <div class="hi4">
        <h2>REFERENCES</h2>
            <p>
            Agrawal, R., Gehrke, J., Gunopulos, D., & Raghavan, P. (1998). Automatic subspace
            clustering of high dimensional data for data mining applications. In Proceedings of the ACM SIGMOD Int’l Conference on Management of Data, pp. 94-105, Seattle,WA.
            </p>
        </div>
        <div>
           <p>
             Bauer, E. & Kohavi, R. (1999). An empirical comparison of voting classification
             algorithms: Bagging, boosting, and variants. Machine Learning, 36:105-139.
             Bradley, P. S., Fayyad, U. M., & Reina, C. (1998). Scaling EM (expectation-maximization)
            clustering to large databases. Technical Report MSR-TR-98-35, Microsoft, Redmond,WA. 
            </p>
        </div>
        <div>
           <p>
         Breiman, L. (1996a). Bagging predictors. Machine Learning, 24(2):123-140.<br>
         Breiman, L. (1996b). Bias, variance, and arching classifiers. Technical Report 460,
            University of California, Department of Statistics, Berkeley, CA.
            </p>
        </div>
        <div>
           <p>
        Breiman, L. (1999). Random forests-Random features. Technical Report 567, University
       of California, Department of Statistics, Berkeley, CA.<br>
          Buhmann, J. (1995). Data clustering and learning. In Arbib, M. (ed.), Handbook of Brain
Theory and Neural Networks. Cambridge, MA: Bradfort Books/MIT Press. <br>
               Cheeseman, P. & Stutz, J. (1996). Bayesian classification system (AutoClass): Theory
and results. In U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, & R. Uthurusamy(eds.),
Advances in Knowledge Discovery and Data Mining, pp. 153-180, San Francisco:
AAAI/MIT Press.<br>
               Chen, S., Guerra-Salcedo, C., & Smith, S. (1999). Non-standard crossover for a standard
representation - Commonality-based feature subset selection. In Proceedings of
the Genetic and Evolutionary Computation Conference, pp. 129-134. San Francisco: Morgan Kaufmann.<br>
               Cunningham, P. & Carney, J. (2000). Diversity versus quality in classification ensembles
based on feature selection. Technical Report TCD-CS-2000-02, Trinity College,
Department of Computer Science, Dublin, Ireland.<br>
               Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Statistical Society, Series B,
39(1):1-38.<br>
               Devaney, M. & Ram, A. (1997). Efficient feature selection in conceptual clustering. In
Proceedings of the 14th Int’l Conference on Machine Learning, pp. 92-97. San
Francisco: Morgan Kaufmann.<br>
               Dy, J. G. & Brodley, C. E. (2000a). Feature subset selection and order identification for
unsupervised learning. In Proceedings of the 17th Int’l Conference on Machine
Learning, pp. 247-254. San Francisco: Morgan Kaufmann.<br>
               Dy, J. G. & Brodley, C. E. (2000b). Visualization and interactive feature selection for
unsupervised data. In Proceedings of the 6th ACM SIGKDD Int’l Conference on
Knowledge Discovery & Data Mining (KDD-00), pp. 360-364, ACM Press.<br>
               Freund, Y. & Schapire, R. (1996). Experiments with a new boosting algorithm. In
Proceedings of the 13th Int’l Conference on Machine Learning, pp. 148-156, Bari,
Italy, Morgan Kaufmann.<br>
               Goldberg, D. E. & Richardson, J. (1987). Genetic algorithms with sharing for multimodal
function optimization. In Proceedings of the 2nd International Conference on
Genetic Algorithms, pp. 41-49. Hillsdale, NJ: Lawrence Erlbaum.<br>
               Guerra-Salcedo, C. & Whitley, D. (1999). Genetic approach to feature selection for
ensemble creation. In GECCO-99: Proceedings of the Genetic and Evolutionary
Computation Conference, pp. 236-243. San Francisco: Morgan Kaufmann.<br>
               Ho, T. K. (1998a). C4.5 decision forests. In Proceedings of the 14th International
Conference on Pattern Recognition, IEEE Computer Society, pp. 545-549.<br>
               Ho, T. K. (1998b). The random subspace method for constructing decision forests. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 20(8):832-844.<br>
               Horn, J. (1997). Multi-criteria decision making and evolutionary computation. In T. Back,
D. B. Fogel & Z. Michaelevicz (Eds.), Handbook of Evolutionary Computation.
London: Institute of Physics Publishing.<br>
               Kim, Y. & Street, W. N. (2000). CoIL challenge 2000: Choosing and explaining likely
caravan insurance customers. Technical Report 2000-09, Sentient Machine Research and Leiden Institute of Advanced Computer Science. http://
www.wi.leidenuniv.nl/~putten/library/cc2000/.<br>
               Mangasarian, O. L., Street, W. N., & Wolberg, W. H. (1995). Breast cancer diagnosis and
prognosis via linear programming. Operations Research, 43(4):570-577.<br>
               Meila, M. & Heckerman, D. (1998). An experimental comparison of several clustering
methods. Technical Report MSR-TR-98-06, Microsoft, Redmond, WA.<br>
               Menczer, F. & Belew, R. K. (1996). From complex environments to complex behaviors.
Adaptive Behavior, 4:317-363.<br>
               Menczer, F., Degeratu, M., & Street, W. N. (2000). Efficient and scalable Pareto optimization by evolutionary local selection algorithms. Evolutionary Computation,
8(2):223-247.<br>
               Menczer, F., Street, W. N., & Degeratu, M. (2000). Evolving heterogeneous neural agents
by local selection. In V. Honavar, M. Patel & K. Balakrishnan(eds.), Advances in
the Evolutionary Synthesis of Intelligent Agents. Cambridge, MA: MIT Press.<br>
               Opitz, D. (1999). Feature selection for ensembles. In Proceedings of the 16th National
Conference on Artificial Intelligence (AAAI), pp. 379-384, Orlando, FL, AAAI.<br>
               Street, W. N. & Kim, Y. (2001). A streaming ensemble algorithm (SEA) for large-scale
classification. In Proceedings of the 7th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining (KDD-01), pp.377-382, ACM Press.<br>
               Street, W. N., Mangasarian, O. L., & Wolberg, W. H. (1995). An inductive learning
approach to prognostic prediction. In A. Prieditis & S. Russell(eds.), Proceedings
of the 12th International Conference on Machine Learning, pp. 522-530, San
Francisco: Morgan Kaufmann.<br>
               Van Veldhuizen, D. A. (1999). Multiobjective evolutionary algorithms: Classifications,
analyses, and new innovations. PhD thesis, Air Force Institute of Technology.

            </p>
        </div>
        <div class="hi5">
            <h3>Chapter V</h3>
        </div>
        <div class="hi6">
            <h2>Parallel and Distributed
Data Mining through
Parallel Skeletons and
Distributed Objects</h2>
        </div>
        <div class="hi7">
            <h5>Massimo Coppola
University of Pisa, Italy</h5>
        </div>
        <div class="hi8">
            <h5>Marco Vanneschi
University of Pisa, Italy</h5>
        </div>
        <div class="hi9">
            <h2>ABSTRACT</h2>
            <p>
                We consider the application of parallel programming environments to develop portable
and efficient high performance data mining (DM) tools. We first assess the need of
parallel and distributed DM applications, by pointing out the problems of scalability
of some mining techniques and the need to mine large, eventually geographically
distributed databases. We discuss the main issues of exploiting parallel and distributed
computation for DM algorithms. A high-level programming language enhances the
software engineering aspects of parallel DM, and it simplifies the problems of integration
with existing sequential and parallel data management systems, thus leading to
programming-efficient and high-performance implementations of applications. We
describe a programming environment we have implemented that is based on the
parallel skeleton model, and we examine the addition of object-like interfaces toward
external libraries and system software layers. This kind of abstractions will be included
in the forthcoming programming environment ASSIST. In the main part of the chapter,
as a proof-of-concept we describe three well-known DM algorithms, Apriori, C4.5, and
DBSCAN. For each problem, we explain the sequential algorithm and a structured
parallel version, which is discussed and compared to parallel solutions found in the
            </p>
        </div>
    </body>
    
</html>    